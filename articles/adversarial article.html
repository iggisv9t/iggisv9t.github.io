<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="generator" content="Zettlr">
  <meta name="date" content="">
  <title>Adversarial Attacks in Real World</title>
  <style type="text/css">
  * {
    box-sizing: border-box;
  }

  a {
    /* color: #991199; */
    text-decoration: none;
  }

  a:hover {
    text-decoration: underline;
  }

  hr {
    border: none;
    border-bottom: 1px solid #999;
    width: 80%;
  }

  html, body {
    margin: 0;
    padding: 0;
  }

  body {
    background-color: white;
    color: #333;
    font-family: 'DejaVu', 'Georgia', 'Times New Roman', 'Times', serif;
  }

  article {
    width: 50%;
    font-size: 1.5em;
    margin: 0 auto;
    line-height: 150%;
  }

  /* Better display on printing */
  @media print {
    article {
      width: 90%;
      font-size: 12pt;
      margin: 0 auto;
      line-height: 150%;
    }
  }

  article p {
    hyphens: auto;
    text-align: justify;
  }

  h1, h2, h3, h4, h5, h6 {
    font-family: 'Raleway', 'Lato', 'Liberation sans', 'Helvetica', sans-serif;
    color: #332233;
  }

  img {
    max-width: 100%;
    height: auto;
  }

  blockquote {
    font-size: 80%;
    color: rgba(120, 120, 120, 1);
    margin: 2% 5%;
    line-height: 120%;
  }

  table {
    border-collapse: collapse;
    width: 100%;
    font-size: 70%;
    font-family: 'Raleway', 'Lato', 'Liberation sans', 'Helvetica', sans-serif;
  }

  th, td {
    padding: 4px 20px;
    border-bottom: 1px solid #333;
  }

  figure {
    display: inline-block;
    align-items: center;
  }

  figure figcaption {
    text-align: center;
  }

  /* List of classes taken from the skylighting lib */
  /* See: https://github.com/jgm/skylighting/blob/master/skylighting-core/src/Skylighting/Format/HTML.hs */

  /* Colors: Solarized theme (light) */
  /* See: https://ethanschoonover.com/solarized/ */

  /* Specific implementation: Taken from the GEdit theme (light) */
  /* See: https://github.com/altercation/solarized/blob/master/gedit/solarized-light.xml */

  /* Generic styles */
  pre.sourceCode {
    color: #657b83;
    background-color: #fdf6e3;
    box-shadow: 0px 0px 20px 0px rgba(0, 0, 0, .05);
    padding: 2px;
    overflow: auto;
  }

  /* All classes, taken from skylighting lib */
  .sourceCode .kw { color: #859900; font-weight: bold; } /* Keyword */
  .sourceCode .dt { color: #b58900; } /* Datatype */
  .sourceCode .df { color: #d33682; } /* Decimal values*/
  .sourceCode .bn { color: #d33682; } /* base-n-integers */
  .sourceCode .fl { color: #d33682; } /* Floats */
  .sourceCode .ch { color: #2aa198; } /* Character */
  .sourceCode .st { color: #2aa198; } /* String */
  .sourceCode .vs { color: #2aa198; } /* Verbatim String */
  .sourceCode .ss { color: #2aa198; } /* Special String */
  .sourceCode .co { color: #586e75; font-style: italic; } /* Comment */
  .sourceCode .ot { font-weight: bold; } /* Other token */
  .sourceCode .al { color: #d33682; background-color: #073642; } /* Alert */
  .sourceCode .fu { color: #268bd2; } /* Function name */
  .sourceCode .re {} /* Region marker */
  .sourceCode .er { color: #dc322f; font-weight: bold; } /* Error */
  .sourceCode .cn { color: #2aa198; } /* Constant */
  .sourceCode .sc { color: #dc322f; } /* Special character */
  .sourceCode .im { color: #6c71c4; font-weight: bold; } /* Import statement */
  .sourceCode .do { color: #dc322f; } /* Documentation string */
  .sourceCode .an { color: #2aa198; } /* Annotation */
  .sourceCode .cv { color: #6c71c4; } /* Comment var */
  .sourceCode .va { color: #268bd2; } /* Variable */
  .sourceCode .cf { color: #859900; } /* Control Flow (if, else, return) */
  .sourceCode .op {} /* Operator */
  .sourceCode .bu { color: #b58900; } /* Builtin function/class/identifier */
  .sourceCode .ex { color: #268bd2; } /* Extension */
  .sourceCode .pp { color: #cb4b16; } /* Preprocessor, like #import in C++ */
  .sourceCode .at { color: #dc322f; } /* Attribute */
  .sourceCode .in { color: #586e75; } /* Information */
  .sourceCode .wa { color: #cb4b16; } /* Warning */
  </style>

<!-- Pandoc variables -->

<!-- Additional CSS in case the user has passed it -->

<!-- Include MathJax CDN, if applicable -->
</head>
<body>
  <!-- Render in article for reader view enabling -->
    <article>
    <!-- Indentation must be zero for, e.g., PREs to work correctly -->
<h1 id="введение">Введение</h1>
<p>В декабре 19-го я <a href="https://t.me/sv9t_channel/439">запостил</a>
 про адверсариал свитшоты. С этого началась серия постов про адверсариал
 атаки в реальном мире. Эта тема вызывала большой интерес, поэтому я 
продолжал её копать. Ушёл я не так далеко, чтобы публиковать какое-то 
исследование, но зато много раз отвечал на вопросы и показывал примеры. 
Главный вывод всей этой деятельности: чудес не бывает. Дальше я обобщу 
всё, о чём писал и отвечу на частозадаваемые вопросы по этой теме.</p>
<h4 id="дисклеймер-1">Дисклеймер 1:</h4>
<p>Если вы занимаетесь компьютерным зрением, или даже adversarial 
attacks and defences – это ваше основное направление, то многого от этой
 статьи не ждите. Тут только поверхностный обзор нескольких статей, 
попытки воспроизведения по готовому коду или с минимальными его 
модификациями.</p>
<h4 id="дисклеймер-2">Дисклеймер 2:</h4>
<p>Я понятия не имею, как работают системы распознавания, например, в 
московском метро или где-то ещё в реальных масштабных применениях, 
соответственно, я не знаю как такие системы обманывать (и тем более не 
призываю к этому).</p>
<h4 id="дисклеймер-3">Дисклеймер 3:</h4>
<p>Это пересказ очень сумбурных и спонтанных копаний темы, поэтому цельный рассказ сложить не получится, хотя я старался.</p>
<h2 id="что-такое-адверсариал-атаки">Что такое адверсариал атаки</h2>
<p>В двух словах (если мы говорим о компьютерном зрении) – это когда в 
изображение вносится какое-то изменение, которое не кажется человеку 
значительным или вовсе не заметно, но нейросеть начинает видеть в 
изменённом изображении что-то совершенно другое. Это очень хорошо 
работает на цифровых изображениях, потому что мы точно можем установить 
значения отдельных пикселей. Если же переносить это в реальный мир, то 
возникают сложности. Даже если напечатать и сфотографировать картинку, 
то меняются яркость, оттенки, углы поворота, теряются какие-то мелкие 
детали. А если говорить о печати на ткани и съёмке движущихся объектов, 
то тут всё ещё хуже.</p>
<h2 id="с-чего-всё-началось">С чего всё началось:</h2>
<figure>
<img src="adversarial%20article_files/551dcc3db8fb2c913ef95f9d52423cf0.png" alt="Причём здесь трубка: см. Вероломство Образов, Рене Магритт."><figcaption aria-hidden="true">Причём здесь трубка: см. Вероломство Образов, Рене Магритт.</figcaption>
</figure>
<p>Начал копать с того, что эта картинка набрала много дизлайков.</p>
<p>Есть знаменитая работа в этой теме <a href="https://arxiv.org/abs/1712.09665">Adversarial Patch</a></p>
 <img src="adversarial%20article_files/a5452c5fa8f6f7a58481dbf07ecfd53f.png" alt="Вся суть работы">
<p>Суть работы изображена в этой картинке. Она наделала больше всего 
шума, так как показала принципиальную возможность таких атак. Её я тоже 
пробовал воспроизвести.</p>
<p><img src="adversarial%20article_files/3243f652cbfdf066ed4ee1d9f601dce2.png" alt="3243f652cbfdf066ed4ee1d9f601dce2.png"> Я проверял и общем-то оно работает, хотя не так красиво, как на демо от авторов. Но я и условия не создавал такие же. (тут <a href="https://t.me/sv9t_channel/530">гифка</a>)</p>
<h2 id="если-нейросети-такие-умные-почему-их-так-легко-обмануть">Если нейросети такие умные, почему их так легко обмануть</h2>
<p>Простите, коллеги по отрасли, за все неточности в этом разделе, т. к.
 я попытаюсь дать общее объяснение для тех, кто считает что нейросеть и 
искусственный интеллект – это одно и то же.</p>
<p>Сначала в общих чертах: большинство алгоритмов машинного обучения 
просто пытаются как-то обобщить примеры, которые им показывали. Если бы 
им показывали все возможные в мире примеры, они бы не ошибались никогда.
 А так, нейронки пытаются самым ленивым путём связать примеры, которые 
им показывают, и правильные ответы. Из-за этого возникают ситуации, 
когда, например, модель запоминает на какую камеру снято изображение и 
совсем не смотрит, что там изображено, потому что так проще определить 
правильный ответ в тренировочных данных. Как бы мы ни старались, все 
данные в мире мы достать не можем, поэтому модели будут уязвимы к 
каким-то тонким деталям, которые люди просто не замечают (<em>Я видел такое, во что вы, люди, просто не поверите</em>).
 Например если рядом с большинством примеров какого-то класса всегда 
было какое-то специфичное сочетание пикселей. Ну просто так сложилось.</p>
<p>Вот тут мы переходим к более глубоким деталям. Нейронки – это не 
модель мозга, хотя там тоже есть элементы, которые называются “нейроны”.
 На самом деле нейроны – это просто наборы чисел, которые умножаются на 
другие числа, а вся нейросеть – это просто набор найденных хитрым 
способом чисел, с которыми можно путём последовательности несложных 
математических операций из входных данных делать нужные выходные данные.
 Тут в основном речь идёт о переходе от значений пикселей к значениям 
классов, например “здесь нарисована собака” с вероятностью 
0.8390827540129. Ничего похожего на мышление тут нет. Поэтому можно 
подобрать такие значения входных данных, что какая-то непонятная ерунда 
для нейросети будет похожа на собаку даже больше, чем любая собака.</p>
<h2 id="статьи-которые-я-читал-и-решения-которые-я-пробовал">Статьи, которые я читал, и решения, которые я пробовал</h2>
<figure>
<img src="adversarial%20article_files/77507f796bf97468093a071e2103362c.png" alt="Чуваки из Facebook Research хвастаются результатами"><figcaption aria-hidden="true">Чуваки из Facebook Research хвастаются результатами</figcaption>
</figure>
<p>Самая впечатляющая статья <a href="https://medium.com/syncedreview/personal-invisibility-cloak-stymies-people-detectors-15bebdcc7943">здесь (пост на медиум)</a> – её чаще всего постят, её обсуждают, в ней самые многообещающие картинки. Сама статья от исследователей здесь <a href="https://arxiv.org/pdf/1910.14667.pdf">pdf на arxiv</a>.
 Могу сразу обратить ваше внимание на то, что у людей в этих свитшотах 
руки спрятаны или прижаты. А ещё они стоят в основном так, что их 
контуры почти сливаются. Разбор деталей будет ближе к концу. Что очень 
расстраивает – к статье нет исходного кода для воспроизведения 
экспериментов. И самих адверсариал патчей в достаточном для печати 
разрешении нет. Поэтому я искал что-то подобное.</p>
<p>Из подобного нашлось несколько разных интересных работ.</p>
<figure>
<img src="adversarial%20article_files/8a56edb2c42a80eb1932064e611d5850.png" alt="Очки, которые ломают распознавание лица"><figcaption aria-hidden="true">Очки, которые ломают распознавание лица</figcaption>
</figure>
<p>Например вот эта про адверсариал очки, которые должны мешать распознаванию лица. Но там <a href="https://github.com/mahmoods01/accessorize-to-a-crime">код</a> написан на матлабе, поэтому я даже не пытался запустить.</p>
<figure>
<img src="adversarial%20article_files/edb18ce5db975624d99d504f92d16326.png" alt="Is This Loss?"><figcaption aria-hidden="true">Is This Loss?</figcaption>
</figure>
<p>Вот <a href="https://arxiv.org/pdf/1906.11897.pdf">такая работа</a>, где патч ставили просто перед камерой, чтобы он отвлекал машину от распознавания всего остального. Кстати, голова скрыта.</p>
<figure>
<img src="adversarial%20article_files/ced681eec6188566ffdf72ab3fa2ba51.png" alt="Ребята, можно мы уже пойдём?"><figcaption aria-hidden="true">Ребята, можно мы уже пойдём?</figcaption>
</figure>
<p>Но самой многообещающей оказалась вот эта статья, у которой есть исходники и тоже красивые картинки, как оно работает. <a href="https://arxiv.org/abs/1904.08653">Статья</a>, <a href="https://gitlab.com/EAVISE/adversarial-yolo">код</a> Её легко запустить и легко опробовать. Этим я и занялся.</p>
<p>Я скачал код, немного его поковырял, чтобы он работал и быстрее сохранял картинки. Сменил сид, обучил патч и напечатал.</p>
<figure>
<img src="adversarial%20article_files/8df2dc45fe06b1fd89b0a2e5b0ed2941.png" alt="Мои патчи"><figcaption aria-hidden="true">Мои патчи</figcaption>
</figure>
<figure>
<img src="adversarial%20article_files/2d5cbbcea12f1b6a360b440af316960e.png" alt="Тут должна быть гифка"><figcaption aria-hidden="true">Тут должна быть гифка</figcaption>
</figure>
<p>И протестировал <a href="https://t.me/sv9t_channel/508">Пост в ТГ</a></p>
<p>Что меня смущало сразу, ещё до теста: патч на демо-видео всегда 
находился на уровне пояса и перекрывал контур человека. Ну ещё капюшон, 
закатанные рукава до уровня верхней границы патча. Ребята на видео 
всегда стояли почти одинаково. Но в целом, если бы оно работало хоть 
как-то – никаких претензий. Это же всё-таки исследование. Что же 
оказалось на самом деле? Ну оно практически не срабатывало. Можно было 
найти хороший ракурс и положение, когда патч заглушал распознавание 
человека (меня в кадре), но надеяться на хоть сколько-то стабильную 
работу не приходилось. Если спрятать руки и голову из кадра, то патч 
работал. Если целиком уйти из кадра, но показать хоть кончики пальцев, 
то всё, с высокой уверенностью детектируется класс <code>person</code>. В
 итоге я смог выдать себя за холодильник, за вазу, сделал несколько 
скриншотов, которые можно было бы вставить в статью для демонстрации 
успеха, но фактически – нет, не работает.</p>
<p>Потом я ещё поковырял код, пообучал по-разному и обнаружил одну 
штуку: когда я делаю какую-то свою инициализацию – оно сходится к 
разным, но похожим картинкам. Когда я подмешиваю совсем чуть-чуть от 
изображения на котором сами авторы проводили демонстрацию – все метрики 
резко улучшаются и итоговая картинка становится почти такой же, как в 
статье. Похоже они нашли инициализацию лучше, чем я и показывали только 
лучший из всех полученных результатов.</p>
<p>Чтобы закрыть тему с той статьёй, я всё же сделал свитшот из того, что смог получить</p>
<figure>
<img src="adversarial%20article_files/f77dbc4d02020a640022523c6aa6e24b.png" alt="Если прятать руки и голову, то работает."><figcaption aria-hidden="true">Если прятать руки и голову, то работает.</figcaption>
</figure>
<figure>
<img src="adversarial%20article_files/0d8624fbf9f5cf4091b2eb0818b0767e.png" alt="С головой уже нет"><figcaption aria-hidden="true">С головой уже нет</figcaption>
</figure>
<p>В защиту статьи скажу, что я не всё сделал по-честному и правильно. 
Во-первых ребята тренировались обманывать YOLOv2, а я тестировал на 
YOLOv3. Не думаю, что они готовы были бы гарантировать какие-то 
результаты в таком случае. Во-вторых у меня очень шакальная веб-камера. 
Как я уже упоминал, другое качество изображения может легко обезвредить 
атаку. В защиту подхода, а не конкретной модели, могу сказать, что они и
 датасет взяли маленький и не похожий на тот, на котором в основном 
тренируют такие модели. Они это оправдывали тем, что в их датасете 
больше людей в полный рост. Но суть то в том, что чем меньше датасет – 
тем легче атаковать модель.</p>
<h1 id="глубже-в-статьи">Глубже в статьи</h1>
<p>Для краткости вот <a href="https://arxiv.org/pdf/1910.14667.pdf">статья</a> - там делают свитшоты с принтами. Буду её называть “статья про свитеры”. И <a href="https://arxiv.org/pdf/1904.08653.pdf">другая</a>
 – тут делают патчи, которыми надо перегородить человека, буду называть 
“статья с патчами” – это у неё есть доступный код и её результаты я 
пытался воспроизвести. <a href="https://t.me/sv9t_channel/560">Пост в ТГ</a></p>
<p>Ну что, к делу. Статья со свитерами, гораздо ближе к реальности и 
даёт ответы на многие вопросы о границах применимости самого подхода. 
Статью с патчами, можно назвать Proof of Concept всей этой темы. До 
этого ещё была статейка где перед камерой вешали патч и он подавлял все 
детекции в кадре – вот она имеет только академический интерес, а 
применить тот подход в жизни очень сложно.</p>
<p>В патчах ребята тренировались на датасете inria и тестировали на 
YOLOv2 обученном на COCO. Чтобы это дело работало не только в цифровом 
виде, но и в реальности, они докидывали случайное изменение яркости и 
поворот до 20 градусов. То есть сетап очень ограниченный и тест тоже 
очень скупой. Но так как они первые такое проворачивали – это очень 
круто. Тем более они опубликовали код и видео теста.</p>
<p>Разумеется, появляются вопросы, будет ли это работать с другими 
датасетами (в inria все люди в полный рост, кажется), будет ли это 
работать если патч будет повёрнут, согнут, баланс белого уедет и так 
далее, и самое главное, будет ли работать с другими детекторами. 
Некоторые ограничения и сложности опущены, и о них я узнал из других 
статей. Например, что хороший патч не обязан получиться с одного прогона
 – надо попробовать несколько инициализаций.</p>
<p>Вот все эти вопросы покрывают мужички в свитерах из фейсбука. Они 
тренировали патчи под разные дететоры, тренировали под ансамбль 
детекторов, сравнивали с серой картинкой, с шумом, с картиной Сёра, с 
серым++ (они так назвали синий, который хорошо портит предикты от YOLO).
 При обучении делали больше искажений и аугментаций, чтобы оно было 
более устойчиво при переносе в реальный мир. Учили на трёх датасетах 
(правда из YOLO они выкинули всё, что не человек, и людей выбрали 10000 
картинок). Тестировали переносимость между датасетами, тестировали как 
будет работать отражение картинки, как будет работать на других классах.
 Там ещё много интересного про то, как они тестировали в физическом мире
 это всё. В общем, постарались и покрыли много случаев. Не все, конечно,
 но много.</p>
<p>Выводы такие, что в физическом мире работает, но значительно хуже. А 
ещё они не опубликовали код и данные, на которых тестировали. Хотя 
учитывая объём проделанной работы, не факт, что они сами смогут быстро 
взять и перезапустить всё, что наэкспериментировали.</p>
<figure>
<img src="adversarial%20article_files/04e8998bb49fa923cba7191b1effdcbb.png" alt="В табличке тест на цифровых изображениях."><figcaption aria-hidden="true">В табличке тест на цифровых изображениях.</figcaption>
</figure>
<figure>
<img src="adversarial%20article_files/bc729825ce3ef95587b4bc22984dadf6.png" alt="На картинках и на живых людях"><figcaption aria-hidden="true">На картинках и на живых людях</figcaption>
</figure>
<p>А вот на бумажках и в реальном мире. Нас интересует больше всего 
правый нижний график. Зелёных палочек мало и они очень короткие, и 
всегда в предпоследнем столбце в каждой группе. Это значит, что 
непосредственно одеждой изредка удаётся обмануть только YOLOv2 и то с не
 очень высокой уверенностью.</p>
<p>Это значит, что у самой мощной работы, которую все кругом цитируют, о
 которой пишут посты с громкими заголовками, результаты на самом деле 
далеко не такие впечатляющие, как об этом кричит заголовок “Invisibilty 
cloak”</p>
<h1 id="выводы">Выводы</h1>
<p><strong>(которые, впрочем, стоит проверить)</strong></p>
<ol type="1">
<li>Руки и голова – очень сильные сигналы. Достаточно одного пальца в кадре, чтобы нейронка задетектировала человека.</li>
<li>Качество изображения, характеристики камеры и все прочие параметры 
очень сильно влияют на успешность атак. Есть даже работа, где просто 
добавляли зашумления в изображение, чтобы сделать модель более 
устойчивой.</li>
<li>Атаки используют в основном связаны с датасетами, а не с моделями</li>
<li>Кажется, работ, которые спасут вас от тотальной слежки и утешат вашу паранойю не существует.</li>
<li>Если для вас детекторы и вообще всякие нейронки – это магия, то не 
пытайтесь защищаться от неё странным колдунством (я намекаю на ту 
историю с гримом от детекторов). Важно понимать детали.</li>
<li>Если выходить за рамки академической работы и/или художественного проекта, то это очень долго и дорого.</li>
<li>Есть шансы, что без всякого диплёрнинга можно сделать оптические иллюзии, которые тоже будут работать как атаки на детекторы.</li>
</ol>
<p>Как мне кажется, какой-то практический смысл может иметь делать шапку
 или маску-респиратор. Шапка во всех ракурсах будет выглядеть примерно 
одинаково, поэтому легко натренировать под неё принт, а маска в любом 
случае затрудняет идентификацию. К тому же, если детектор теряет голову,
 то он обычно теряет и всего остального человека (я не проверял, всегда 
ли так, но у меня срабатывало без всяких адверсариал патчей).</p>
<h1 id="прочие-заметки">Прочие заметки</h1>
<p><strong>В основном просто копипасты моих постов близких к теме</strong></p>
<p>…<br>
Я видел примеры как делают грим против распознавания лиц. Это очень 
сомнительная практика и больше похоже на шамнаство: против волшебных и 
загадочных алгоритмов будем бороться ритуальной раскраской. Может быть 
это работает на каких-то отдельных простых системах, но в целом не 
работает. К тому же привлекает сильно много внимания.</p>
<figure>
<img src="adversarial%20article_files/708b84b0cc7fdaa9036209b05f0135db.png" alt="ВАС ЗАМЕТИЛИ"><figcaption aria-hidden="true">ВАС ЗАМЕТИЛИ</figcaption>
</figure>
<p>…<br>
У меня есть гипотеза, почему авторы той статьи, где стикеры ломали 
классификацию банана, выбрали тостер как целевой класс. Это такое 
распространённое обзывательство для роботов в англоязычной среде. Вот 
даже нагуглил, что в научной фантастике это почти <a href="https://scifi.stackexchange.com/questions/151202/what-was-the-first-example-of-toaster-being-a-robot-slur">традиция</a></p>
<p>…<br>
Оказалось, что на нипсе была как раз работа где классификатор учили на 
зашумлённых изображениях и потом на предикте делали вотинг 
классификаторов с разным шумом добавленным на входе. Получался 
устойчивый к атакам классификатор. Прямо как моя история с плохой вебкой
 <a href="https://t.me/sv9t_channel/530">Пост в ТГ</a></p>
<p>Вот тут <a href="https://github.com/locuslab/smoothing">код</a> с хорошим tl;dr с картинками и гифками. В заголовке ссылка на статью. <a href="https://t.me/sv9t_channel/536">Пост в ТГ</a></p>
<p>…<br>
По-настоящему физическая атака – <a href="https://www.reflectacles.com/phantom">очки, которые отражают ИК</a>
 и на камере лицо вообще не видно. Работать будет только ночью, когда 
камеры включают ИК подстветку и снимают ИК фильтры. Дорого только. 
Возможно самому найти подходящие материалы должно быть просто. 
Наколхозить камеру, которая видит в ИК довольно просто из обычной 
пластиковой бутылки камеры. <a href="https://t.me/sv9t_channel/629">Пост в ТГ</a></p>
<p>…<br>
Искал статейки, чтобы снова начать копать тему с адверсариал атаками и 
нашёл забавную штуку. Тут про атаку на системы распознавания речи. Вот <a href="https://youtu.be/l_AkXxZt10I?t=388">видео с объяснением</a>.
 Привязка по времени к месту, где собираются воспроизводить примеры. А 
так примеры можно послушать здесь https://adversarial-attacks.net/ — там
 кроме самого первого примера можно доскроллить до ещё четырёх. И это 
очень напоминает какой-то старый фильм про будущее. Шум, который 
накладывают на оригинальное аудио звучит правда, как речь роботов из 
фильмов. Ну и юморок от исследователей забавный. Меня больше всех 
впечатлил пример с музыкой, потому что там адверсариал-шум можно легко 
принять просто за плохое качество аудио.</p>
<p>Из-за картинок и названия “Psychoacoustic Hiding” сначала подумал, что там какая-то антинаука. <a href="https://t.me/sv9t_channel/643">Пост в ТГ</a></p>
<p>…<br>
Ещё пробовал <a href="https://arxiv.org/pdf/1910.11099.pdf">вот эту штуку</a>.
 Там была многообещающая идея – искажения снять с модельной футболки и 
потом по ним моделировать искажения патча при тренировке. Но оказалось, 
что эта штука не работает вообще никак.</p>
<p>И у них есть свой <a href="https://www.bonfire.com/store/adversarial-apparel/">магазин</a> (не покупайте, оно не работает):</p>
<p>…<br>
<a href="https://towardsdatascience.com/fooling-facial-detection-with-fashion-d668ed919eb">Маска, которая ломает детекцию лица на Histogram of Oriented Gradients</a></p>
<h1 id="что-ещё-можно-попробовать">Что ещё можно попробовать</h1>
<p>Попробовать учить на нормальных больших датасетах и повторять сетап 
ребят с фейсбук АИ. Можно сделать модели реальных искажений, засняв на 
людях модельные толстовки с решёткой на ткани. Учить с шумом, на 
картинках разного качества и против разных моделей параллельно. Можно 
перейти на шапки, маски и перчатки – то есть глушить сразу самые сильные
 сигналы.</p>
<p>Можно перестать пытатьтся делать физические атаки и делать атаки на 
соцсети. Например инстаграм и фейсбук может рассказать, что он видит на 
фото, если правильно спросить.</p>
<h1 id="ссылки-на-все-посты">Ссылки на все посты</h1>
<p>По ходу моих копаний в теме</p>
<p><a href="https://t.me/sv9t_channel/439">0</a>, <a href="https://t.me/sv9t_channel/442">1</a>, <a href="https://t.me/sv9t_channel/443">2</a>, <a href="https://t.me/sv9t_channel/442">3</a>, <a href="https://t.me/sv9t_channel/456">4</a>, <a href="https://t.me/sv9t_channel/460">5</a>, <a href="https://t.me/sv9t_channel/490">6</a>, <a href="https://t.me/sv9t_channel/491">7</a>, <a href="https://t.me/sv9t_channel/492">8</a>, <a href="https://t.me/sv9t_channel/506">9</a>, <a href="https://t.me/sv9t_channel/507">10</a>, <a href="https://t.me/sv9t_channel/508">11</a>, <a href="https://t.me/sv9t_channel/509">12</a>, <a href="https://t.me/sv9t_channel/510">13</a>, <a href="https://t.me/sv9t_channel/511">14</a>, <a href="https://t.me/sv9t_channel/512">15</a>, <a href="https://t.me/sv9t_channel/527">16</a>, <a href="https://t.me/sv9t_channel/528">17</a>, <a href="https://t.me/sv9t_channel/530">18</a>, <a href="https://t.me/sv9t_channel/531">19</a>, <a href="https://t.me/sv9t_channel/532">20</a>, <a href="https://t.me/sv9t_channel/536">21</a>, <a href="https://t.me/sv9t_channel/553">22</a>, <a href="https://t.me/sv9t_channel/559">23</a>, <a href="https://t.me/sv9t_channel/560">24</a>, <a href="https://t.me/sv9t_channel/561">25</a>, <a href="https://t.me/sv9t_channel/562">26</a>, <a href="https://t.me/sv9t_channel/563">27</a>, <a href="https://t.me/sv9t_channel/564">28</a>, <a href="https://t.me/sv9t_channel/565">29</a>, <a href="https://t.me/sv9t_channel/566">30</a>, <a href="https://t.me/sv9t_channel/583">31</a>, <a href="https://t.me/sv9t_channel/584">32</a>, <a href="https://t.me/sv9t_channel/602">33</a>, <a href="https://t.me/sv9t_channel/603">34</a>, <a href="https://t.me/sv9t_channel/604">35</a>, <a href="https://t.me/sv9t_channel/605">36</a>, <a href="https://t.me/sv9t_channel/629">37</a>, <a href="https://t.me/sv9t_channel/643">38</a>, <a href="https://t.me/sv9t_channel/655">39</a></p>
  </article>


</body></html>